from copy import deepcopy
import rospy
import numpy as np
import torch
from torch.optim import Adam
import time
import datetime
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from src.envs.core import Env
import src.ddpg.core as core
import src.util.rb as rb
from src.ddpg.icm import ICM
from src.util.core import EpochDataSaver
from std_msgs.msg import Float32, String, Empty, Float32MultiArray

def ddpg(seed=0, start_epochs=0, max_epochs=200, save_freq_epochs=200, random_epochs=10, greedy_epochs=50,
		 max_epoch_steps=100, start_update_steps=512, update_freq_steps=50, batch_size=64,
		 replay_size=int(1e6), gamma=0.99, polyak=0.995, pi_lr=1e-4, q_lr=1e-3,
		 act_noise=0.1, e_greedy=0.95, with_attention=True, with_dueling=True, with_lstm=False,
		 with_icm=True, with_ac=True, icm_lr=1e-4, icm_lambda_f=0.2, icm_lambda_i=1):

	torch.manual_seed(seed)
	np.random.seed(seed)

	rospy.init_node('stage_0_ddpg')

	env = Env()
	eds = EpochDataSaver()

	ac = core.ActorCritic(env.obs_dim, env.scan_dim, env.act_dim, with_attention, with_dueling, with_lstm)

	file_name = ''
	if file_name != '':
		ac.load_state_dict(torch.load('/home/zsw/catkin_ws/src/turtlebot3_drl_pp/results/'+file_name+'.pt'))
		print('DRL_Model: Load model from', file_name)

	if start_epochs!=0:
		eds.load_data_file(file_name)

	ac_targ = deepcopy(ac)

	replay_buffer = rb.ReplayBuffer(obs_dim=env.obs_dim, act_dim=env.act_dim, size=replay_size)

	if with_icm:
		icm = ICM(env.obs_dim, env.scan_dim)
		icm_optimizer = Adam(icm.parameters(), lr=icm_lr)
		if with_ac:
			ac_record_init = False
			ac_record_count = 0
			min_icm_pr = 0
			avg_icm_pr = 0
			max_icm_pr = 0
			print('ICM_Module: Load AC-ICM Module')
		else:
			print('ICM_Module: Load ICM Module')

	# Set up optimizers for policy and q-function
	pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)
	q_optimizer = Adam(ac.q.parameters(), lr=q_lr)

	# Freeze target networks with respect to optimizers (only update via polyak averaging)
	for p in ac_targ.parameters():
		p.requires_grad = False

	# Set up function for computing DDPG Q-loss
	def compute_loss_q(data):
		o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']
		q = ac.q(o,a)
		# Bellman backup for Q function
		with torch.no_grad():
			q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))
			backup = r + gamma * (1 - d) * q_pi_targ
		# MSE loss against Bellman backup
		loss_q = ((q - backup)**2).mean()
		q_value = q.detach().numpy().mean()
		return loss_q, q_value
	
	def update_ac_record(pr):
		nonlocal min_icm_pr
		nonlocal max_icm_pr
		nonlocal avg_icm_pr
		nonlocal ac_record_count
		nonlocal ac_record_init
		if not ac_record_init:
			min_icm_pr = pr
			max_icm_pr = pr
			avg_icm_pr = pr
			ac_record_init = True
		else:
			if pr < min_icm_pr:
				min_icm_pr = pr
			if pr > max_icm_pr:
				max_icm_pr = pr
			avg_icm_pr = (avg_icm_pr * ac_record_count + pr) / (ac_record_count + 1)
		ac_record_count += 1

	def compute_ac_intrinsic_reward(x, *y):
		nonlocal min_icm_pr
		nonlocal max_icm_pr
		nonlocal avg_icm_pr
		a = (avg_icm_pr - min_icm_pr) / 2
		b = (max_icm_pr - avg_icm_pr) / 2
		if avg_icm_pr - a <= x <= avg_icm_pr:
			x = 0.5 * (x - avg_icm_pr + a) / a
		elif avg_icm_pr < x <= avg_icm_pr + b:
			x = 0.5 + 0.5 * (x - avg_icm_pr) / b
		else:
			x = 0
		return x
	
	def compute_loss_q_with_icm(data):
		o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']
		q = ac.q(o,a)

		pred_a, pred_phi2, phi2 = icm(o, o2, a)
		
		# pred_errors = ((pred_phi2 - phi2.detach())**2).sum(axis=1)
		pred_errors = ((pred_phi2.detach() - phi2.detach())**2).mean(1)
		if with_ac:
			# update ac record
			prs = torch.split(pred_errors, 1)
			for pr in prs:
				update_ac_record(pr.squeeze().clone())
			# compute ac reward
			pred_errors.map_(pred_errors, compute_ac_intrinsic_reward)
		intrinsic_reward = icm_lambda_i * pred_errors

		loss_forward_model = ((pred_phi2 - phi2.detach())**2).mean()
		loss_inverse_model = ((pred_a - a.detach())**2).mean()
		loss_icm = (1 - icm_lambda_f) * loss_inverse_model + icm_lambda_f * loss_forward_model

		# Bellman backup for Q function
		with torch.no_grad():
			q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))
			backup = r + gamma * (1 - d) * q_pi_targ + intrinsic_reward

		# MSE loss against Bellman backup
		loss_q = ((q - backup)**2).mean()
		q_value = q.detach().numpy().mean()
		return loss_icm, loss_q, q_value

	# Set up function for computing DDPG pi loss
	def compute_loss_pi(data):
		o = data['obs']
		q_pi = ac.q(o, ac.pi(o))
		return -q_pi.mean()

	def update(data):
		if with_icm:
			icm_optimizer.zero_grad()
			q_optimizer.zero_grad()
			
			loss_icm, loss_q, q_value = compute_loss_q_with_icm(data)
			
			loss_icm.backward()
			loss_q.backward()

			icm_optimizer.step()
			q_optimizer.step()
		else:
			# First run one gradient descent step for Q.
			q_optimizer.zero_grad()
			
			loss_q, q_value = compute_loss_q(data)
			
			loss_q.backward()
			
			q_optimizer.step()

		# Freeze Q-network so you don't waste computational effort 
		# computing gradients for it during the policy learning step.
		for p in ac.q.parameters():
			p.requires_grad = False

		# Next run one gradient descent step for pi.
		pi_optimizer.zero_grad()
		loss_pi = compute_loss_pi(data)
		loss_pi.backward()
		pi_optimizer.step()

		# Unfreeze Q-network so you can optimize it at next DDPG step.
		for p in ac.q.parameters():
			p.requires_grad = True

		# Finally, update target networks by polyak averaging.
		with torch.no_grad():
			for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):
				# NB: We use an in-place operations "mul_", "add_" to update target
				# params, as opposed to "mul" and "add", which would make new tensors.
				p_targ.data.mul_(polyak)
				p_targ.data.add_((1 - polyak) * p.data)
	
	def get_action(o, noise, epsilon):
		if np.random.uniform() >= epsilon:
			random_vel = []
			lin_vel = np.random.uniform(0,1)
			random_vel.append(lin_vel)
			ang_vel = np.random.uniform(-1,1)
			random_vel.append(ang_vel)
			return random_vel
		else:
			a = ac.act(torch.as_tensor(o, dtype=torch.float32))
			a += noise * np.random.randn(env.act_dim)
			return np.clip(a, [0, -1], [1, 1])

	total_steps = 0

	for e in range(max_epochs-start_epochs):
		ep_start_time = time.time()
		ep_ret, ep_len, ep_time = 0, 0, 0
		o, dif = env.reset()
		d = False
		while(d == False):
			if start_epochs+e+1 <= random_epochs:
				a = get_action(o, act_noise, 0)
			elif random_epochs < start_epochs+e+1 <= greedy_epochs:
				a = get_action(o, act_noise, e_greedy)
			else:
				a = get_action(o, act_noise, 1)
			o2, dif2, r, d, dr = env.step(a)
			ep_ret += r
			ep_len += 1
			total_steps += 1
			if ep_len == max_epoch_steps and d == False:
				d = True
				dr = 'overstep'
			replay_buffer.store(o, a, r, o2, d)

			if with_ac:
				with torch.no_grad():
					pred_a, pred_phi2, phi2 = icm(torch.Tensor(o), torch.Tensor(o2), torch.Tensor(a))
					pr = ((pred_phi2 - phi2)**2).mean()
					update_ac_record(pr.clone())

			o = o2
			dif = dif2
			if total_steps >= start_update_steps and total_steps % update_freq_steps == 0:
				for _ in range(update_freq_steps):
					batch = replay_buffer.sample_batch(batch_size)
					update(data=batch)
		
		ep_time = round(time.time() - ep_start_time, 4)
		ep_goal = '(' + str(env.goal.goal_x) + ',' + str(env.goal.goal_y) + ')'
		eds.push_data(ep_goal, ep_ret, ep_len, ep_time, dr)
		rospy.loginfo('EpochLogger: epoch=%s ep_ret=%f ep_len=%s ep_time=%s done_reason=%s', start_epochs+e+1, ep_ret, ep_len, str(ep_time), dr)
		
		if (start_epochs+e+1) %	save_freq_epochs == 0:
			name = 'stage_0_train_'+ str(start_epochs+e+1) + 'epochs_'+datetime.datetime.now().strftime("%Y-%m-%d-%H-%M")
			# save drl model
			torch.save(ac.state_dict(), '/home/zsw/catkin_ws/src/turtlebot3_drl_pp/results/'+name+'_acm.pt')
			# save exp data
			eds.save_data_file(name)
			eds.save_data_excel(name)
	
	env.reset(end=True)

if __name__ == '__main__':
	ddpg()
